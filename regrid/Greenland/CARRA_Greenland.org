#+TITLE: CARRA WorkBook - Greenland (GAP FILLING)
#+AUTHOR: Ken Mankoff
#+EMAIL: kdm@geus.dk
#+DATE: {{{time(%Y-%m-%d)}}}

#+PROPERTY: header-args:sh :tangle-mode (identity #o755) :shebang "#!/bin/bash" :comments org

* Introduction

This workbook shows how to convert JEB ASCII albedo products to GeoTIFF files.

** Requirements

+ A =lon.csv= and =lat.csv= file containing the locations of the data. These files are simply columns of data. Example:

#+BEGIN_SRC sh :results verbatim
head lon.csv
#+END_SRC
#+RESULTS:
#+begin_example
     -120.355
     -120.318
     -120.280
     -120.243
     -120.206
     -120.168
     -120.131
     -120.094
     -120.056
     -120.019
#+end_example

+ Data files, located anywhere, but then in folders and named <year>/<year>_<doy>.txt. For example, here we use =./input/=, and then the required sub-folders and file names. Example:

#+BEGIN_SRC sh :results verbatim
find ./input/ | head -n15
#+END_SRC
#+RESULTS:
#+begin_example
./input/
./input//2015
./input//2015/2015_093.txt
./input//2015/2015_094.txt
./input//2015/2015_095.txt
./input//2015/2015_096.txt
./input//2015/2015_097.txt
./input//2015/2015_098.txt
./input//2016
./input//2016/2016_060.txt
./input//2016/2016_061.txt
./input//2016/2016_062.txt
./input//2016/2016_063.txt
./input//2016/2016_064.txt
./input//2016/2016_065.txt
#+end_example

+ The GNU =parallel= utility.

#+BEGIN_SRC sh :results verbatim
parallel --version
#+END_SRC
#+RESULTS:
#+begin_example
GNU parallel 20180522
Copyright (C) 2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018
Ole Tange and Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
GNU parallel comes with no warranty.

Web site: http://www.gnu.org/software/parallel

When using programs that use GNU Parallel to process data for publication
please cite as described in 'parallel --citation'.
#+end_example

+ The BedMachine v3 data set in the current folder (symlink is fine)

#+BEGIN_SRC sh :results verbatim
ln -s ~/data/Greenland/Morlighem_2017/BedMachineGreenland-2017-09-20.nc 
ls -l ./BedMachineGreenland-2017-09-20.nc 
#+END_SRC

#+RESULTS:
: lrwxr-xr-x  1 kdm  staff  74 Jul 11 12:05 ./BedMachineGreenland-2017-09-20.nc -> /Users/kdm/data/Greenland/Morlighem_2017/BedMachineGreenland-2017-09-20.nc


** Running The Code

See [[./CARRA_pre.sh]], [[./CARRA_daily.sh]], and [[./CARRA_post.sh]] for the code. At the top of each file are examples how to run them. These three files must be run sequentially and not in parallel for any given data set.

* Pre-process: Setup Environment
:PROPERTIES:
:header-args: :tangle CARRA_pre.sh
:END:

Run this setup code 1x as:

Generic example:
=grass74 -c <projection> <GRASS_FOLDER> --exec ./CARRA_pre.sh=

Specific example:
=grass74 -c EPSG:3413 ./GRASS_GREENLAND --exec ./CARRA_pre.sh=

#+BEGIN_SRC sh
#+END_SRC

** Convert (lon,lat) to (x,y)
#+BEGIN_SRC sh :results verbatim

if [[ ! -e xy.txt ]]; then
  time paste -d"|" lon.csv lat.csv | m.proj -i input=- | cut -d"|" -f1,2 > xy.txt
  # real 2m20.927s
  # user 3m32.692s
  # sys	1m4.286s
  head xy.txt
fi

#+END_SRC
#+RESULTS:

** Set the region to the bounds of the provided data

=r.in.xyz= requires a 3rd column (z), so I just replicate the file and give it 4 colums: x|y|x|y. It uses the 2nd =x= as =z= (but ignores it) and ignores the 2nd =y=.

#+BEGIN_SRC sh :results verbatim
# paste -d"|" xy.txt xy.txt | r.in.xyz -sg input=-         # print 
time eval $(paste -d"|" xy.txt xy.txt | r.in.xyz -sg input=-) # set e,w,n,s variables in the shell
g.region e=$e w=$w s=$s n=$n                             # set bounds in GRASS
g.region res=500 -pal                                    # set resolution and print
g.region -s                                              # save as default region
#+END_SRC
#+RESULTS:

** Generate an ocean mask based on the data

We generate an ocean mask because we will fill in small missing parts (holes in the data due to the reprojection - not everything is actually at 5 km x 5 km, some cells have 2 points averaged and others have 0 points) with a 3x3 smooth. But we don't want to grow all land masses into the sea, so we build an ocean mask here and then apply it after the smooth, pushing the expanded land back to the coast.

+ read in one day of data
+ set 0 to null
+ clump
+ find largest clump (the ocean)
+ build mask equal to largest clump
+ export for use elsewhere maybe
#+BEGIN_SRC sh :results verbatim
time paste -d"|" xy.txt xy.txt | r.in.xyz --q input=- output=mask_MODIS --o
r.mapcalc "mask_MODIS = if(isnull(mask_MODIS), 0, 1)" --o
time r.clump input=mask_MODIS output=clumps --o
# manual inspect
# r.stats -c clumps sort=desc | head
time clump_ID_largest=$(r.stats -c clumps sort=desc | head -n1 | cut -d" " -f1)
r.mapcalc "mask_ocean = if(clumps == ${clump_ID_largest}, null(), 1)" --o
# r.out.gdal -cm input=mask_ocean output=mask_ocean.tif type=Byte createopt="COMPRESS=DEFLATE" --o
#+END_SRC

** Generate a filter

We do a 3x3 smooth to fill in small missing parts before we do the big gap-filling. Here is the kernel for the 3x3 smooth.

#+BEGIN_SRC sh :results verbatim
cat << EOF > ./filter.txt
TITLE     See r.mfilter manual
    MATRIX    3
    1 1 1
    1 1 1
    1 1 1
    DIVISOR   0
    TYPE      P
EOF
#+END_SRC
#+RESULTS:


** Apply the filter on the MODIS mask
Fill in all the small holes just as we would on the daily data. Then the "missing" regions should only be the big holes.
#+BEGIN_SRC sh :results verbatim
r.null map=mask_MODIS setnull=0
r.mfilter -z input=mask_MODIS output=mask_MODIS_fill filter=./filter.txt --o
r.mapcalc "mask_MODIS = mask_MODIS_fill * mask_ocean" --o
#+END_SRC
#+RESULTS:

** Find missing regions
NASA data is missing over some regions. In order to fill in the missing albedo we find the regions, and for each region find a 100 km buffer around it. When processing the daily data we will find valid albedo in this 100 km buffer, correlate it with elevation, and then fill in the missing data based on its elevation.

We'll do this work in a different mapset

#+BEGIN_SRC sh :results verbatim
g.mapset -c missing

# Read in the land/sea/ice mask and elevation
time r.import input=netCDF:./BedMachineGreenland-2017-09-20.nc:mask output=mask_BedMachine --o
r.colors map=mask_BedMachine color=random
r.import input=netCDF:./BedMachineGreenland-2017-09-20.nc:surface output=z_s

# find where there is no albedo data but there is land or ice
r.mapcalc "missing = if(isnull(mask_MODIS) & (! isnull(mask_BedMachine)), 1, null())" --o
r.colors map=missing color=blue
r.clump input=missing output=missing_clumps --o

# remove all small missing areas, less than X hectares
# frink "5 km * 5 km -> hectare"
# 2500
# Lets limit to 25 grid cells; 2500*25 = 62500
# Lets limit to 10 grid cells; 2500*10 = 25000

r.reclass.area -c input=missing_clumps output=missing_clumps_area value=25000 mode=greater method=reclass --o

# In the loop below, for each missing clump number <n>, generate a clump_<n> and a clump_<n>_buffer mask. Later (each day), we'll loop over each of the clump_<n>_buffer, find the relationship for that day and area between albedo and elevation, and apply it to the clump_<n> for that day.

g.region -d
for a in $(r.stats -n missing_clumps_area); do # for each (large) area
  r.mapcalc "clump_${a} = if(missing_clumps_area == ${a}, 1, null())" --o
  g.region zoom=clump_${a}
  g.region e=e+100000 w=w-100000 s=s-100000 n=n+100000 # expand by +- 100 km
  r.buffer input=clump_${a} output=clump_${a}_buffer distances=100 units=kilometers --o --verbose --o
  g.region -d
done
g.mapset PERMANENT

#+END_SRC
#+RESULTS:


* Daily work
:PROPERTIES:
:header-args: :tangle CARRA_daily.sh
:END:

Run one day like this (generic example):
=grass74 -c <GRASS_FOLDER>/<MAPSET> --exec ./CARRA_daily.sh </path/to/input/YYYY/YYYY_DOY.txt> </path/to/output/=

Use the same =<GRASS_FOLDER>= you created when running =./CARRA_pre.sh=. A specific example is:

=grass74 -c ./GRASS_GREENLAND/DAILY --exec ./CARRA_daily.sh ./input/2016/2016_060.txt ./out/=

And =./out/2016/2016_060.tif= will be created.

Run all days in parallel like this (pipe list of files to =parallel=):

=find ./input -name "*.txt" | parallel --bar grass74 -c ./GRASS_GREENLAND/{%} --exec ./CARRA_daily.sh {.} ./out/=

Run one year in parallel like this (same as above but =grep= for the year):

=find ./input -name "*.txt" | grep 2016 | parallel --bar grass74 -c ./GRASS_GREENLAND/{%} --exec ./CARRA_daily.sh {.} ./out/=

#+BEGIN_SRC sh
#+END_SRC

** Generate GeoTIFFs for one day
+ Read in each day
+ Apply quick filter
+ Fill gaps
+ Record where data is stale missing
+ Export
#+BEGIN_SRC sh :results verbatim

### DEBUGGING. COMMENT OUT WHEN RUNNING
# input=./input/2016/2016_060.txt
### WHEN RUNNING, GET VARS FROM CLI ARGS
input=${1}
out=${2}
f=$(dirname ${input} | rev | cut -d"/" -f2- | rev) # input folder
y=$(basename ${input} | cut -d"_" -f1)
d=$(basename ${input} .txt | cut -d"_" -f2)

g.region -d  # use default resolution
r.mask -r

mkdir -p ${out}/${y}

# Project data
echo "reading in data (~30-60s)..."
paste -d"|" xy.txt ${f}/${y}/${y}_${d}.txt | r.in.xyz --q input=- output=day type=CELL --o

# Fill in small holes
r.null map=day setnull=0
r.mfilter -z input=day output=day_fill filter=./filter.txt --o
# undo the coastline growth
r.mapcalc "day = (day_fill / 10000) * mask_ocean" --o


# fill in large clumps
# MCB = "missing clump buffer"
# MC = "missing clump"
# mcb=clump_1477_buffer # DEBUG
# mcb=clump_972_buffer # DEBUG
for mcb in $(g.list type=raster mapset=missing pattern="clump_*_buffer"); do
  mc=$(echo $mcb | cut -d"_" -f1,2)
  r.mask -r --q
  g.region raster=${mcb}@missing
  g.region zoom=${mcb}@missing
  r.mask raster=${mcb}@missing --o
  # r.mapcalc "MASK = ${mcb}@missing * (day > 0.3)" --o
  # r.regression.line mapx=z_s mapy=${y}_${d} --verbose
  eval $(r.regression.line -g mapx=z_s@missing mapy=day --verbose)
  echo "Correlation: $R"
  r.mapcalc "patch_${mc} = if(isnull(${mc}@missing), day, ((${a})+(${b}*z_s@missing)))" --o
done
  
g.region -d
r.mask -r

r.patch -s --o input=day,$(g.list type=raster pattern=patch_* separator=,) output=day_filled
r.mfilter -z input=day_filled output=day_filled_filter filter=./filter.txt --o
r.out.gdal --q -fcm input=day_filled_filter output=${out}/${y}/${y}_${d}.tif type=Float32 createopt="COMPRESS=DEFLATE" --o
#+END_SRC
#+RESULTS:


* Post-process
:PROPERTIES:
:header-args: :tangle CARRA_post.sh
:END:

Run with:
=grass74 -c ./GRASS_GREENLAND/PERMANENT --exec ./CARRA_post.sh /path/to/output/=

#+BEGIN_SRC sh
#+END_SRC

** Climatological mean for all years

#+BEGIN_SRC sh :results verbatim

out=$1

g.mapset -c climatology
mkdir ${out}/climatology
# d=060 # debug 
for d in $(seq -w 060 274); do
  g.remove -f type=raster pattern=*
  for year in $(seq 2000 2006); do
    fname=${out}/${year}/${year}_${d}.tif
    r.external input=${fname} output=year_${year} --o
  done

  addlist=$(g.list separator="+" type=raster pattern=year_[0-9]*)
  n=$(echo $addlist | tr '+' '\n' | wc -l)
  r.mapcalc "avg = float(${addlist})/${n}" --o
  r.out.gdal -cm input=avg output=${out}/climatology/2000-2006_${d}.tif type=Float32 createopt="COMPRESS=DEFLATE" --o
done
#+END_SRC

** Minimum value for each year
#+BEGIN_SRC sh :results verbatim

out=$1

g.mapset -c minimum
mkdir ${out}/min

for year in $(seq 2000 2017); do
  g.remove -f type=raster pattern=*

  seq -w 366 | parallel --bar r.external input=${out}/${year}/${year}_{.}.tif output=day_{.} --o

  minlist=$(g.list separator=comma type=raster pattern=day_[0-9]*)
  r.mapcalc "min = min(${minlist})" --o
  r.out.gdal input=min output=${out}/min/${year}.tif createopt="COMPRESS=DEFLATE" type=Float32 --o
done
#+END_SRC
#+RESULTS:

** Staleness
#+BEGIN_SRC sh :results verbatim

out=$1

g.mapset -c stale
mkdir ${out}/stale
for year in $(seq 2000 2017); do
  g.remove -f type=raster pattern=*
  mkdir ${out}/stale/${year}

  # read in all days for this year
  seq -w 060 274 | parallel --bar r.external input=${out}/${year}/${year}_{.}.tif output=day_{.} --o

  r.mapcalc "stale = -1" --o # initial value everywhere
  for d in $(seq -w 061 274); do
    d0=$(echo "$d-1"|bc -l)
    d0=$(printf %03G $d0)

    # if the data changed from yesterday, set stale to 0. Otherwise,
    # set it to stale+1, unless it was -1 (no valid data yet) in which
    # case keep it at -1
    r.mapcalc "stale = if(day_${d} != day_${d0}, 0, if(stale == -1, -1, stale+1))" --o
    r.out.gdal -cm input=stale output=${out}/stale/${year}/${year}_${d}.tif type=Int16 createopt="COMPRESS=DEFLATE" --o
  done
done
#+END_SRC

* Runner

#+BEGIN_SRC sh :results verbatim :tangle runner.sh

# setup
grass74 -c EPSG:3413 ./GRASS_GREENLAND --exec ./CARRA_pre.sh

# # Run 2012 and 2017
# find /mnt/ice/Jason/MOD10A1/Greenland/to_regrid/2012/ -name "*.txt" | parallel --bar grass74 -c ./GRASS_GREENLAND/{%} --exec ./CARRA_daily.sh {.} /mnt/ice/Ken/CARRA/Greenland

# find /mnt/ice/Jason/MOD10A1/Greenland/to_regrid/2017/ -name "*.txt" | parallel --bar grass74 -c ./GRASS_GREENLAND/{%} --exec ./CARRA_daily.sh {.} /mnt/ice/Ken/CARRA/Greenland

# Run the rest
for y in $(seq 2003 2016 | grep -v 2012); do 
    find /mnt/ice/Jason/MOD10A1/Greenland/to_regrid/${y}/ -name "*.txt" | parallel --bar grass74 -c ./GRASS_GREENLAND/{%} --exec ./CARRA_daily.sh {.} /mnt/ice/Ken/CARRA/Greenland
done

grass74 -c ./GRASS_GREENLAND/PERMANENT --exec ./CARRA_post.sh /mnt/ice/Ken/CARRA/Greenland
#+END_SRC
#+RESULTS:
